2016-neural-computers:
  doi: 10.1038/nature20101
  tags: [machine-learning, deep-learning]
  description: |+
    Augment deep networks with an external memory (RAM) matrix.

    Bart says: "TL;DR: This work follows a line of research that teaches deep-nets
    to learn algorithmic tasks (addition, sorting, multiplication, key-value look-up).
    This paper goes a bit further and teaches their network to do shortest-path finding
    in graphs and demonstrates on maps of the London underground. Cool demo with nice results,
    but the hype-machine has blown it out of proportion
    (check out the FT article for a breathless take claiming thinking computers are one step closer...)"

2017-deep-lift:
  arxiv: "1704.02685"
  tags: [machine-learning, deep-learning]
  description: |+
    Decompose ouput predictions

2016-guttenberg-deep-slow:
  arxiv: "1609.00116"
  tags: [machine-learning, deep-learning]
  description: |+
    Somehow uses deep networks to extract slow modes from dynamical signals.

2016-aspuru-mol-feat:
    arxiv: "1610.02415"
    tags: [machine-learning, cheminformatics, misc]
    description: |+
        The authors train an auto-encoder to provide a vector representation
        for small molecules. Small molecules are graphs with varying
        sizes, so they're hard to feed into neural nets (which require
        fixed-length bitvectors). By fusing together an encoder and decoder
        (and making the "middle" representation sufficiently small), they
        learn a vector representation.

        The authors lean heavily on [arxiv:1511.06349=25] to autoencode
        SMILES strings.

        They use a variational autoencoder (noisy) to avoid "dead zones"
        in latent space.

        They optomize OLED properties as an example.

"arxiv:1511.06349":
    arxiv: "1511.06349"
    tags: [misc]
    description: |+
        Advances in autoencoding text, used by [2016-aspuru-mol-feat].
