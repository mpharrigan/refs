2016-msm-cryptic-binding:
  doi: 10.1038/ncomms12965
  tags: [misc]
  description: |+
    They generated ensembles using MD, then docked to those ensembles, then re-weighted the docking scores
    based on the MSM. This gave a huge improvement in the predictive power of docking to predict affinity/potency.
    It turned an inverse relationship (when docking using xtal structures) into a highly correlated trend.

    They confirmed their hypothesis about the protein flexibility by using a mass spec. method.

    They identified a loop movement important in the anti-antibacterial activity of the enzyme that was different
    from one previously proposed/suspected.

    They proposed mutants that would stabilize their proposed loop, and tested them experimentally.

    The power of using the MSM to re-weight other analyses is also very encouraging
    to see yet again. Also note that they did all this with what looks like a pretty low amount of aggregate
    sampling (few microseconds per mutant).

2016-mbar-volumes:
  doi: 10.1103/PhysRevE.94.031301
  tags: [misc]
  description: |+
    Use multistate benett acceptance (MBAR) to find volumes in high dimensions.

2016-neural-computers:
  doi: 10.1038/nature20101
  tags: [machine-learning, deep-learning]
  description: |+
    Augment deep networks with an external memory (RAM) matrix.

    Bart says: "TL;DR: This work follows a line of research that teaches deep-nets
    to learn algorithmic tasks (addition, sorting, multiplication, key-value look-up).
    This paper goes a bit further and teaches their network to do shortest-path finding
    in graphs and demonstrates on maps of the London underground. Cool demo with nice results,
    but the hype-machine has blown it out of proportion
    (check out the FT article for a breathless take claiming thinking computers are one step closer...)"

2017-deep-lift:
  arxiv: "1704.02685"
  tags: [machine-learning, deep-learning]
  description: |+
    Decompose ouput predictions

2016-guttenberg-deep-slow:
  arxiv: "1609.00116"
  tags: [machine-learning, deep-learning]
  description: |+
    Somehow uses deep networks to extract slow modes from dynamical signals.

2016-aspuru-mol-feat:
    arxiv: "1610.02415"
    tags: [machine-learning, cheminformatics, misc]
    description: |+
        The authors train an auto-encoder to provide a vector representation
        for small molecules. Small molecules are graphs with varying
        sizes, so they're hard to feed into neural nets (which require
        fixed-length bitvectors). By fusing together an encoder and decoder
        (and making the "middle" representation sufficiently small), they
        learn a vector representation.

        The authors lean heavily on [arxiv:1511.06349=25] to autoencode
        SMILES strings.

        They use a variational autoencoder (noisy) to avoid "dead zones"
        in latent space.

        They optomize OLED properties as an example.

"arxiv:1511.06349":
    arxiv: "1511.06349"
    tags: [misc]
    description: |+
        Advances in autoencoding text, used by [2016-aspuru-mol-feat].
