2017-baker-antibody-design:
  doi: 10.1038/nbt.3907
  description: |+
    Uses computation to design an antibody for influenza A

2014-ganguli-saddle-points:
  arxiv: "1406.2572"
  tags: [misc, machine-learning, deep-learning]
  description: |+
    Labmate summarizes:

    This one is a really cool paper.  One of those "we've all been doing it wrong" papers that could have a big
    impact.  Their main conclusions are

    1. When optimizing functions in high dimensional spaces, saddle points are a much bigger problem than local
    minima.  There are far more of them, and the few local minima that do exist mostly have values only slightly
    worse than the global minimum.

    2. Standard optimization methods deal really badly with saddle points (and hence work really badly in high
    dimensional spaces).  First order methods like gradient descent start taking tiny steps, so they take a really
    long time to escape.  Quasi-Newton methods are even worse.  They just converge to the saddle point and never escape.

    3. They describe a new approach that doesn't have these problems and goes right through saddle points without
    slowing down.

    They do all this in the context of neural networks, but it likely applies just as well to other high dimensional
    optimization problems.  Proteins, for example.  When you use an algorithm like L-BFGS for energy minimization,
    it's probably converging to a saddle point, not a local minimum.  It could be really interesting to try their
    method.  Could we fold a protein to the native state just by a straightforward energy minimization?

    Force field optimization is another case whether this approach could be really useful.

    They also show that at a saddle point, there's a strong monotonic relationship between the error and the
    fraction of negative eigenvalues of the Hessian.  Potentially that could be used as a way to measure how
    far you are from the global minimum.  For example, when optimizing force field parameters, it would tell you
    whether your parameters are close to optimal, or whether there's still a lot of room to improve them further.

2016-msm-cryptic-binding:
  doi: 10.1038/ncomms12965
  tags: [misc]
  description: |+
    Labmate summarizes:

    They generated ensembles using MD, then docked to those ensembles, then re-weighted the docking scores
    based on the MSM. This gave a huge improvement in the predictive power of docking to predict affinity/potency.
    It turned an inverse relationship (when docking using xtal structures) into a highly correlated trend.

    They confirmed their hypothesis about the protein flexibility by using a mass spec. method.

    They identified a loop movement important in the anti-antibacterial activity of the enzyme that was different
    from one previously proposed/suspected.

    They proposed mutants that would stabilize their proposed loop, and tested them experimentally.

    The power of using the MSM to re-weight other analyses is also very encouraging
    to see yet again. Also note that they did all this with what looks like a pretty low amount of aggregate
    sampling (few microseconds per mutant).

2016-mbar-volumes:
  doi: 10.1103/PhysRevE.94.031301
  tags: [misc]
  description: |+
    Use multistate benett acceptance (MBAR) to find volumes in high dimensions.

2016-neural-computers:
  doi: 10.1038/nature20101
  tags: [machine-learning, deep-learning]
  description: |+
    Augment deep networks with an external memory (RAM) matrix.

    Bart says: "TL;DR: This work follows a line of research that teaches deep-nets
    to learn algorithmic tasks (addition, sorting, multiplication, key-value look-up).
    This paper goes a bit further and teaches their network to do shortest-path finding
    in graphs and demonstrates on maps of the London underground. Cool demo with nice results,
    but the hype-machine has blown it out of proportion
    (check out the FT article for a breathless take claiming thinking computers are one step closer...)"

2017-deep-lift:
  arxiv: "1704.02685"
  tags: [machine-learning, deep-learning]
  description: |+
    Decompose ouput predictions

2016-guttenberg-deep-slow:
  arxiv: "1609.00116"
  tags: [machine-learning, deep-learning]
  description: |+
    Somehow uses deep networks to extract slow modes from dynamical signals.

2016-aspuru-mol-feat:
    arxiv: "1610.02415"
    tags: [machine-learning, cheminformatics, misc]
    description: |+
        The authors train an auto-encoder to provide a vector representation
        for small molecules. Small molecules are graphs with varying
        sizes, so they're hard to feed into neural nets (which require
        fixed-length bitvectors). By fusing together an encoder and decoder
        (and making the "middle" representation sufficiently small), they
        learn a vector representation.

        The authors lean heavily on [arxiv:1511.06349=25] to autoencode
        SMILES strings.

        They use a variational autoencoder (noisy) to avoid "dead zones"
        in latent space.

        They optomize OLED properties as an example.

"arxiv:1511.06349":
    arxiv: "1511.06349"
    tags: [misc]
    description: |+
        Advances in autoencoding text, used by [2016-aspuru-mol-feat].
