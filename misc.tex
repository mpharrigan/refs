

% This file is autogenerated. Don't edit it
\documentclass{article}

\usepackage{amsmath}              % need for subequations
\usepackage{amssymb}              % for symbols
\usepackage{hyperref}             % put links on stuff
\usepackage{cleveref}             % use for referencing figures/equations
                                  % hyperref must come before cleverref
\usepackage{natbib}               % biblio
\usepackage{color}                % use if color is used in text
\usepackage{soul}                 % hyphenation, strikethrough
\usepackage{graphicx}
\usepackage{parskip}

\bibliographystyle{unsrt}

\begin{document}

\title{A collection of papers}
\date{\today}

\author{gitbib}

\maketitle

\section{Section title tbd}
\subsection{Computational design of trimeric influenza-neutralizing proteins targeting the hemagglutinin receptor binding site}

This paper can be cited with id \texttt{2017-baker-antibody-design}
\cite{2017-baker-antibody-design}.


Uses computation to design an antibody for influenza A




\subsection{Learning Important Features Through Propagating Activation Differences}

This paper can be cited with id \texttt{2017-deep-lift}
\cite{2017-deep-lift}.


Decompose ouput predictions




\subsection{Hybrid computing using a neural network with dynamic external memory}

This paper can be cited with id \texttt{2016-neural-computers}
\cite{2016-neural-computers}.


Augment deep networks with an external memory (RAM) matrix.

Bart says: "TL;DR: This work follows a line of research that teaches
deep-nets to learn algorithmic tasks (addition, sorting, multiplication,
key-value look-up). This paper goes a bit further and teaches their network
to do shortest-path finding in graphs and demonstrates on maps of the
London underground. Cool demo with nice results, but the hype-machine has
blown it out of proportion (check out the FT article for a breathless take
claiming thinking computers are one step closer...)"




\subsection{Automatic chemical design using a data-driven continuous representation
  of molecules}

This paper can be cited with id \texttt{2016-aspuru-mol-feat}
\cite{2016-aspuru-mol-feat}.


The authors train an auto-encoder to provide a vector representation for
small molecules. Small molecules are graphs with varying sizes, so they're
hard to feed into neural nets (which require fixed-length bitvectors). By
fusing together an encoder and decoder (and making the "middle"
representation sufficiently small), they learn a vector representation.

The authors lean heavily on [arxiv:1511.06349=25] to autoencode SMILES
strings.

They use a variational autoencoder (noisy) to avoid "dead zones" in latent
space.

They optomize OLED properties as an example.




\subsection{Modelling proteinsâ€™ hidden conformations to predict antibiotic resistance}

This paper can be cited with id \texttt{2016-msm-cryptic-binding}
\cite{2016-msm-cryptic-binding}.


Labmate summarizes:

They generated ensembles using MD, then docked to those ensembles, then
re-weighted the docking scores based on the MSM. This gave a huge
improvement in the predictive power of docking to predict affinity/potency.
It turned an inverse relationship (when docking using xtal structures) into
a highly correlated trend.

They confirmed their hypothesis about the protein flexibility by using a
mass spec. method.

They identified a loop movement important in the anti-antibacterial
activity of the enzyme that was different from one previously
proposed/suspected.

They proposed mutants that would stabilize their proposed loop, and tested
them experimentally.

The power of using the MSM to re-weight other analyses is also very
encouraging to see yet again. Also note that they did all this with what
looks like a pretty low amount of aggregate sampling (few microseconds per
mutant).




\subsection{Structural analysis of high-dimensional basins of attraction}

This paper can be cited with id \texttt{2016-mbar-volumes}
\cite{2016-mbar-volumes}.


Use multistate benett acceptance (MBAR) to find volumes in high dimensions.




\subsection{Neural Coarse-Graining: Extracting slowly-varying latent degrees of
  freedom with neural networks}

This paper can be cited with id \texttt{2016-guttenberg-deep-slow}
\cite{2016-guttenberg-deep-slow}.


Somehow uses deep networks to extract slow modes from dynamical signals.




\subsection{Generating Sentences from a Continuous Space}

This paper can be cited with id \texttt{arxiv:1511.06349}
\cite{arxiv:1511.06349}.


Advances in autoencoding text, used by [2016-aspuru-mol-feat].


\subsection{Identifying and attacking the saddle point problem in high-dimensional
  non-convex optimization}

This paper can be cited with id \texttt{2014-ganguli-saddle-points}
\cite{2014-ganguli-saddle-points}.


Labmate summarizes:

This one is a really cool paper.  One of those "we've all been doing it
wrong" papers that could have a big impact.  Their main conclusions are

1. When optimizing functions in high dimensional spaces, saddle points are
a much bigger problem than local minima.  There are far more of them, and
the few local minima that do exist mostly have values only slightly worse
than the global minimum.

2. Standard optimization methods deal really badly with saddle points (and
hence work really badly in high dimensional spaces).  First order methods
like gradient descent start taking tiny steps, so they take a really long
time to escape.  Quasi-Newton methods are even worse.  They just converge
to the saddle point and never escape.

3. They describe a new approach that doesn't have these problems and goes
right through saddle points without slowing down.

They do all this in the context of neural networks, but it likely applies
just as well to other high dimensional optimization problems.  Proteins,
for example.  When you use an algorithm like L-BFGS for energy
minimization, it's probably converging to a saddle point, not a local
minimum.  It could be really interesting to try their method.  Could we
fold a protein to the native state just by a straightforward energy
minimization?

Force field optimization is another case whether this approach could be
really useful.

They also show that at a saddle point, there's a strong monotonic
relationship between the error and the fraction of negative eigenvalues of
the Hessian.  Potentially that could be used as a way to measure how far
you are from the global minimum.  For example, when optimizing force field
parameters, it would tell you whether your parameters are close to optimal,
or whether there's still a lot of room to improve them further.







\bibliography{misc.bib}

\end{document}